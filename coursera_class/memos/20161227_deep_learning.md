# Lec3 reading.

## Page 2
* Q1: What is perceptron? -- need to have more precise definition. 
* Q2: *multi-layer* neural network vs. *multi-layer* perceptrons

## Page 3
* not getting closer to *good weights*, but getting closer to *the target*
* Q3: Need to understand Contex problem?

## Page 4
* Linear neurons == Linear filters

## Page 10 
* Q: Why not (y^n - t^n)^2 ?
* Batch delta rule.

## Page 12
* Online delta rule
* Choosing learning rate is annoying

## Page 10
* I don't think I still get how the gradient is gained.

## Page 16
* Q: what is *the minimum* here?
* Q: gradient vector gained against what?

## Page 17
* Logistic neuron
  * https://en.wikipedia.org/wiki/Logistic_function
  * Derivative of fraction funciton
    http://economia.tistory.com/43

## Page 21
* Delta rule derivative
* Page 10 has the definition for delta-rule.
* Q: what is the logistic unit?

## Page 23
* Q: hand-coded feature?

## Page 24 
* learning by perturbation.

## Page 25
* Q: not sure about trasition from learning by perturbation to backpropagation.

## Page 26
* Error derivatives for hidden units vs. Error derivatives for weights 

## Page 27
* Q: y^i vs. y^j

## Page 28
* *NOTE:* Q: not sure about equations here.

## Page 30
* converting error derivative into a learning procedure.
* dE/dw is a directional vector.
* Two issues
  * Optimization issues
  * Generalization issues

## Page 31
* Q: what is the steepest descent?

## Page 32
* Q: what is *overfitting*? is overfitting is related to generalization issue?

sigmoid neuron is logistic neuron?
